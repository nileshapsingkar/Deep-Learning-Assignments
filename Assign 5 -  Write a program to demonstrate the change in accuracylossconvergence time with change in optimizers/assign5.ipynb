{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess CIFAR-100 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained VGG16 model as a base\n",
    "base = VGG16(include_top=False, input_shape=(32, 32, 3))\n",
    "base.trainable = False  # Freeze the base model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1200, activation=\"relu\"),\n",
    "    layers.Dense(100, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with optimizer: adam\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 3s/step - accuracy: 0.1216 - loss: 4.0122 - val_accuracy: 0.2495 - val_loss: 3.0879\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilesh\\anaconda3\\envs\\mllab\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 626ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2495 - val_loss: 3.0879\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 3s/step - accuracy: 0.2772 - loss: 2.9580 - val_accuracy: 0.3008 - val_loss: 2.8525\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 583ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3008 - val_loss: 2.8525\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 3s/step - accuracy: 0.3204 - loss: 2.7378 - val_accuracy: 0.3224 - val_loss: 2.7385\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 628ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3224 - val_loss: 2.7385\n",
      "Epoch 7/10\n",
      "\u001b[1m17/50\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 2s/step - accuracy: 0.3444 - loss: 2.6133"
     ]
    }
   ],
   "source": [
    "optimizers = [\"adam\", \"sgd\", \"adagrad\", \"rmsprop\", \"nadam\"]\n",
    "for optimizer in optimizers:\n",
    "    print(f\"\\nTraining with optimizer: {optimizer}\\n\")\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit(\n",
    "        train_images, train_labels,\n",
    "        epochs=10,\n",
    "        validation_data=(test_images, test_labels),\n",
    "        steps_per_epoch=50\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"\\nFinal Test Loss: {test_loss}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
